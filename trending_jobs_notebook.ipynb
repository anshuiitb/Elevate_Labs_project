{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8959d91c",
   "metadata": {},
   "source": [
    "\n",
    "# Trending Jobs Scraper + Predictor\n",
    "This notebook scrapes job titles from public job boards (RemoteOK, WeWorkRemotely), fetches Google Trends data for candidate job keywords (using `pytrends`), performs basic NLP clustering and frequency analysis, and uses a simple linear regression on Google Trends interest to **predict which jobs are likely to trend next week**.\n",
    "\n",
    "**Important:** This notebook is a starting point. When you run scrapers, obey each site's `robots.txt` and terms of service. Some sites may block automated scraping; in production use, prefer official APIs or data partners.\n",
    "\n",
    "Files created by this notebook: none by default. You can modify it to save CSVs of scraped job listings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe7d57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install required packages (run once)\n",
    "import sys\n",
    "import subprocess\n",
    "packages = ['requests','beautifulsoup4','pandas','scikit-learn','pytrends','nbformat','numpy','matplotlib']\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--quiet'] + packages)\n",
    "print('Installed packages (or they were already present).')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc665e14",
   "metadata": {},
   "source": [
    "## 1) Scrape Remote job boards (examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553cf403",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Scrape RemoteOK job titles (example)\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "def scrape_remoteok():\n",
    "    url = 'https://remoteok.com/remote-dev-jobs'\n",
    "    headers = {'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0'}\n",
    "    r = requests.get(url, headers=headers, timeout=15)\n",
    "    r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    job_rows = soup.find_all('tr', {'class':'job'})\n",
    "    titles = []\n",
    "    for job in job_rows:\n",
    "        t = job.find('h2')\n",
    "        if t:\n",
    "            titles.append(t.get_text(strip=True))\n",
    "    return pd.DataFrame({'source':'remoteok','title':titles})\n",
    "df_remoteok = scrape_remoteok()\n",
    "print('RemoteOK:', len(df_remoteok), 'jobs scraped')\n",
    "df_remoteok.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee91ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Scrape WeWorkRemotely job titles (example)\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "def scrape_wwr():\n",
    "    url = 'https://weworkremotely.com/categories/remote-programming-jobs'\n",
    "    headers = {'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0'}\n",
    "    r = requests.get(url, headers=headers, timeout=15)\n",
    "    r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    sections = soup.find_all('section', {'class':'jobs'})\n",
    "    titles = []\n",
    "    for sec in sections:\n",
    "        for li in sec.find_all('li', recursive=False):\n",
    "            a = li.find('a', recursive=True)\n",
    "            if a:\n",
    "                h = a.find('span', {'class':'title'})\n",
    "                if h:\n",
    "                    titles.append(h.get_text(strip=True))\n",
    "    return pd.DataFrame({'source':'weworkremotely','title':titles})\n",
    "df_wwr = scrape_wwr()\n",
    "print('WeWorkRemotely:', len(df_wwr), 'jobs scraped')\n",
    "df_wwr.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa98146",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Combine and preprocess titles\n",
    "import pandas as pd\n",
    "df = pd.concat([df_remoteok, df_wwr], ignore_index=True)\n",
    "df['title_clean'] = df['title'].str.replace('[^A-Za-z0-9 &+-]',' ', regex=True).str.lower().str.strip()\n",
    "df['title_clean'] = df['title_clean'].str.replace('\\s+',' ', regex=True)\n",
    "print('Total scraped titles:', len(df))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4291ce04",
   "metadata": {},
   "source": [
    "## 2) NLP clustering and frequency analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164fa738",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NLP: TF-IDF + KMeans clustering to find common job clusters/phrases\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=2000, stop_words='english')\n",
    "X = vectorizer.fit_transform(df['title_clean'].fillna(''))\n",
    "k = 6  # number of clusters (adjustable)\n",
    "km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "labels = km.fit_predict(X)\n",
    "df['cluster'] = labels\n",
    "# top terms per cluster\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "cluster_terms = {}\n",
    "for i in range(k):\n",
    "    top_terms = [terms[ind] for ind in order_centroids[i, :10]]\n",
    "    cluster_terms[i] = top_terms\n",
    "cluster_terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f358503f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Frequency count of bigrams/unigrams in titles to get candidate job keywords\n",
    "from collections import Counter\n",
    "import re\n",
    "def get_ngrams(texts, n=2):\n",
    "    counts = Counter()\n",
    "    for t in texts:\n",
    "        tokens = re.findall(r'\\w+', t)\n",
    "        for i in range(len(tokens)-n+1):\n",
    "            counts[' '.join(tokens[i:i+n])] += 1\n",
    "    return counts\n",
    "unigrams = get_ngrams(df['title_clean'].tolist(), n=1)\n",
    "bigrams = get_ngrams(df['title_clean'].tolist(), n=2)\n",
    "top_unigrams = unigrams.most_common(50)\n",
    "top_bigrams = bigrams.most_common(50)\n",
    "pd.DataFrame(top_bigrams, columns=['bigram','count']).head(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158580e7",
   "metadata": {},
   "source": [
    "## 3) Google Trends (pytrends) and forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c56c210",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use pytrends to fetch Google Trends interest_over_time for candidate keywords\n",
    "from pytrends.request import TrendReq\n",
    "pytrends = TrendReq(hl='en-US', tz=360)\n",
    "# Build candidate keywords from top bigrams (you can pick top 8-12 to avoid API limits)\n",
    "candidates = [x[0] for x in top_bigrams[:10]]\n",
    "print('Candidate keywords for Google Trends:', candidates)\n",
    "pytrends.build_payload(candidates, timeframe='now 7-d')  # last 7 days\n",
    "iot = pytrends.interest_over_time()\n",
    "if iot.empty:\n",
    "    print('No Google Trends data returned. Check network or pytrends limitations.')\n",
    "else:\n",
    "    display(iot.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f592c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Simple prediction: linear regression on interest_over_time to forecast next 7 days\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "predictions = {}\n",
    "if not iot.empty:\n",
    "    for col in iot.columns:\n",
    "        if col == 'isPartial': continue\n",
    "        series = iot[col].fillna(0).values\n",
    "        X = np.arange(len(series)).reshape(-1,1)\n",
    "        y = series\n",
    "        if len(series) >= 3:\n",
    "            lr = LinearRegression().fit(X,y)\n",
    "            future_X = np.arange(len(series), len(series)+7).reshape(-1,1)\n",
    "            pred = lr.predict(future_X).clip(min=0)\n",
    "            predictions[col] = float(pred.mean())  # mean predicted interest next week\n",
    "# Rank candidate keywords by predicted interest\n",
    "pred_df = pd.DataFrame(list(predictions.items()), columns=['keyword','predicted_interest']).sort_values('predicted_interest', ascending=False)\n",
    "pred_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25f875f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save results to CSV if needed\n",
    "pred_df.to_csv('predicted_trending_jobs.csv', index=False)\n",
    "df.to_csv('scraped_job_titles.csv', index=False)\n",
    "print('Saved scraped_job_titles.csv and predicted_trending_jobs.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661ae0cd",
   "metadata": {},
   "source": [
    "\n",
    "## Notes & Next steps\n",
    "- This notebook is a **template**. Depending on target sites, you may need to adapt selectors or use official APIs (LinkedIn, Indeed often block scraping).\n",
    "- For more robust forecasting, consider using Prophet, ARIMA, or collect longer historic Google Trends series (e.g., last 90 days) rather than 7-day windows.\n",
    "- Respect robots.txt and site terms; scraping large volumes can get your IP blocked.\n",
    "- You can extend scraping to other job boards (Indeed, Glassdoor, LinkedIn) but they may require authentication or have anti-bot protections.\n",
    "- If you want, I can adapt this notebook to target specific sites and produce a runnable .ipynb tailored to those sources.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
